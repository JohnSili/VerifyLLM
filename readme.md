<div align="center">

# VerifyLLM: LLM-Based Pre-Execution Task Plan Verification for Robots
A novel framework that combines Large Language Models with Linear Temporal Logic for systematic pre-execution verification of robotic task plans. ğŸ¤–

> [Danil S. Grigorev](https://github.com/)<sup>1,2</sup>, [Alexey K. Kovalev](https://github.com/)<sup>1,3</sup>, [Aleksandr I. Panov](https://github.com/)<sup>1,3</sup>
> 
> MIPT<sup>1</sup>, Pyatigorsk State University<sup>2</sup>, AIRI<sup>3</sup>

[\[ğŸ“„Paper\]](https://arxiv.org/)  [\[ğŸ”¥Project Page\]](https://verifyllm.github.io/) [\[ğŸš€ Quick Start\]](#-quick-start) [\[âœ… Performance\]](#-performance)

[\[ğŸ”§Installation\]](#-installation) [\[ğŸ“Š Datasets\]](#-datasets-coming-soon)

</div>

## ğŸ“‹ Abstract

Modern robotic planning systems generate action sequences that appear correct at first glance but contain hidden errors that only become evident during execution. To address this challenge, we propose **VerifyLLM** â€” a framework that combines Large Language Models (LLMs) with Linear Temporal Logic (LTL) for systematic pre-execution verification of robotic task plans.

Our approach consists of two key steps: (i) **Translation Module** that converts natural language instructions into LTL formulas, and (ii) **Verification Module** that analyzes action sequences using LLM reasoning enhanced by formal constraints. VerifyLLM identifies and corrects three critical types of plan inconsistencies: position errors, missing prerequisites, and redundant actions.

We evaluate VerifyLLM on specialized datasets with LTL annotations, achieving **40% reduction in order errors** and **2.6x better similarity** to ground truth plans compared to baseline methods.

## ğŸ¯ Key Features

- **ğŸ”„ Two-Stage Architecture**: LTL Translation â†’ LLM Verification for comprehensive plan analysis
- **ğŸ¯ Three Error Types**: Identifies position errors, missing prerequisites, and redundant actions
- **ğŸ“ Formal Constraints**: Uses Linear Temporal Logic for temporal dependency modeling
- **ğŸ” Sliding Window Analysis**: Optimal context window of 5 actions for efficient processing
- **ğŸ  Household Domain**: Tested on household robotic tasks
- **ğŸ“ˆ Significant Improvements**: 40% reduction in ordering errors over baselines

## ğŸ† Performance

| Method | LCS Similarity | Missing Actions | Extra Actions | Order Errors |
|--------|---------------|-----------------|---------------|--------------|
| Baseline (Llama-3.2-1B) | 0.0717 | 10.28 | 9.14 | 16.48 |
| CoT Optimizer | 0.0705 | 10.38 | 9.35 | 16.80 |
| VerifyLLM (Llama) | 0.0982 | 11.18 | 9.13 | 15.12 |
| **VerifyLLM (Claude)** | **0.183** | **10.17** | **8.32** | **9.47** |

### Key Improvements
- **40%** reduction in order errors compared to best baseline
- **2.6x** better LCS similarity to ground truth
- **Consistent improvements** across all error types

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- Anthropic API key (for Claude) or OpenAI API key
- PyTorch 1.7.1+

### Installation

1. **Clone the repository**:
```bash
git clone https://github.com/your-username/verifyllm
cd verifyllm
```

2. **Create virtual environment**:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**:
```bash
pip install -r requirements.txt
```

4. **Set up environment variables**:
```bash
# Create .env file
ANTHROPIC_API_KEY=your-api-key
```

### Basic Usage

```python
from src.pipeline.action_processor import ActionProcessor
from src.utils.logging import print_sequence_comparison

# Initialize the processor
processor = ActionProcessor(api_key='your-api-key')

# Define task and original plan
task = "Give milk to cat"
original_plan = [
    "Walk to home office",
    "Walk to bedroom", 
    "Walk to bedroom",
    "Walk to home office"
]

# Process and verify the plan
result = processor.verify_plan(original_plan, task)

if result.success:
    print("âœ… Verification successful!")
    print(f"Original: {len(original_plan)} actions")
    print(f"Optimized: {len(result.optimized_plan)} actions")
    print_sequence_comparison(original_plan, result.optimized_plan)
```

## ğŸ“ Project Structure

```
verifyllm/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models.py                 # Data models and types
â”‚   â”œâ”€â”€ ltl/                      # LTL translation components
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ translator.py
â”‚   â”œâ”€â”€ optimization/             # Sequence optimization
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ context_window.py
â”‚   â”œâ”€â”€ pipeline/                 # Main processing pipeline
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ action_processor.py
â”‚   â””â”€â”€ utils/                    # Utility functions
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ logging.py
â”œâ”€â”€ datasets/                     # Specialized datasets
â”‚   â”œâ”€â”€ alfred_ltl/              # ALFRED-LTL dataset
â”‚   â””â”€â”€ virtualhome_ltl/         # VirtualHome-LTL dataset
â”œâ”€â”€ experiments/                  # Experimental scripts
â”œâ”€â”€ requirements.txt             # Dependencies
â””â”€â”€ README.md                    # This file
```

## ğŸ”§ Installation

### Dependencies
- `anthropic>=0.18.0` (for Claude)
- `torch>=1.7.1`
- `spot>=3.2.0` (for LTL validation)
- `rich>=13.0.0` (for console output)

## ğŸ“Š Datasets (Coming Soon)

### ALFRED-LTL
- **Source**: Derived from ALFRED dataset
- **Tasks**: Household instruction following
- **LTL Annotations**: Temporal constraints and dependencies

### VirtualHome-LTL  
- **Source**: Adapted from VirtualHome dataset
- **Tasks**: Human-like daily activities
- **LTL Annotations**: Common-sense temporal relationships

### Dataset Format
```json
{
  "task_id": "make_tea_001",
  "instruction": "Make a cup of tea",
  "original_plan": ["heat water", "prepare cup", "pour tea", "add sugar"],
  "ltl_formula": "F(heat_water) âˆ§ F(prepare_cup) âˆ§ F(pour_tea)",
  "issues": ["missing_prerequisite", "position_error"],
  "corrected_plan": ["heat water", "prepare cup", "add tea bag", "pour hot water", "add sugar"]
}
```

## ğŸ” Algorithm Details

### Translation Module
1. **Input**: Natural language task description
2. **Process**: Few-shot prompting with LTL examples
3. **Validation**: Syntax checking using Spot library
4. **Output**: Formal LTL formula with atomic propositions

### Verification Module
1. **Input**: Action sequence + LTL formula
2. **Process**: Sliding window analysis (window size: 5) with LLM reasoning
3. **Detection**: Three error types identification
4. **Correction**: Reordering, insertion, removal operations

## ğŸ“„ Citation

```bibtex
@article{grigorev2024verifyllm,
  title={VerifyLLM: LLM-Based Pre-Execution Task Plan Verification for Robots},
  author={Grigorev, Danil S. and Kovalev, Alexey K. and Panov, Aleksandr I.},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2024}
}
```

## ğŸ™ Acknowledgments

- Built upon the foundations of classical planning and modern LLM research
- Special thanks to the ALFRED and VirtualHome dataset creators

---

<div align="center">

**[ğŸ” Back to Top](#verifyllm-llm-based-pre-execution-task-plan-verification-for-robots)**

</div>
